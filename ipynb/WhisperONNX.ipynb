{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110283,"status":"ok","timestamp":1715665510176,"user":{"displayName":"Sky Kim","userId":"06825592219644319438"},"user_tz":-540},"id":"sSDOngglBk_O","outputId":"79619245-9169-47ec-d7d7-c2fbadaf9a0c"},"outputs":[],"source":["#https://github.com/openai/whisper\n","\n","!pip install onnx\n","!pip install onnxruntime\n","!pip install torch\n","!pip install git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57858,"status":"ok","timestamp":1715665568030,"user":{"displayName":"Sky Kim","userId":"06825592219644319438"},"user_tz":-540},"id":"yfZALjuo-_Md","outputId":"f549b002-e30e-451e-c46b-f808d63f317a"},"outputs":[],"source":["import time\n","import whisper\n","import torch\n","\n","# allow to inject real data\n","if \"load_audio_mfc\" not in globals():\n","    load_audio_mfc = lambda: torch.randn(1, 80, 3000)\n","\n","model = whisper.load_model(\"small\") #tiny, base, small, medium, large, tiny.en, base.en, small.en, medium.en\n","model.requires_grad_(False)\n","model.eval()\n","\n","tokenizer = whisper.decoding.get_tokenizer(\n","    model.is_multilingual,\n","    task=\"transcribe\",\n","    language=\"en\"\n",")\n","\n","# x_mel shape: [batch, coeff=80, time=3000]\n","x_mel = load_audio_mfc().to('cpu')\n","\n","# encode the audio\n","# x_audio shape: [batch, time=1500, feature=512]\n","start = time.time()\n","x_audio = model.encoder(x_mel.to('cpu'))\n","\n","# initialize using the start sequence\n","# x_tokens shape: [batch, seq<=448]\n","x_tokens = torch.tensor(\n","    [tokenizer.sot_sequence_including_notimestamps],\n","    dtype=torch.long,\n",").to('cpu')\n","\n","max_tokens = 448\n","next_token = tokenizer.sot\n","while x_tokens.shape[1] <= max_tokens and next_token != tokenizer.eot:\n","    y_tokens = model.decoder(x_tokens, x_audio)\n","\n","    next_token = y_tokens[0, -1].argmax()\n","    x_tokens = torch.concat(\n","        [x_tokens, next_token.reshape(1, 1)],\n","        axis=1,\n","    )\n","\n","print(\"took\", time.time() - start, \"seconds\")\n","print(tokenizer.decode(x_tokens[0]))\n","\n","#: ONNX export\n","torch.onnx.export(\n","    model.encoder,\n","    (x_mel,),\n","    \"encoder.onnx\",\n","    input_names=[\"x\"],\n","    output_names=[\"out\"],\n","    dynamic_axes={\n","        \"x\": {0: \"batch\"},\n","        \"out\": {0: \"batch\"},\n","    },\n",")\n","\n","torch.onnx.export(\n","    model.decoder,\n","    (x_tokens, x_audio),\n","    \"decoder.onnx\",\n","    input_names=[\"tokens\", \"audio\"],\n","    output_names=[\"out\"],\n","    dynamic_axes={\n","        \"tokens\": {0: \"batch\", 1: \"seq\"},\n","        \"audio\": {0: \"batch\"},\n","        \"out\": {0: \"batch\", 1: \"seq\"},\n","    },\n",")\n","\n","#: Execute the ONNX model\n","import numpy as np\n","import onnxruntime\n","\n","sess_encoder = onnxruntime.InferenceSession(\"encoder.onnx\")\n","sess_decoder = onnxruntime.InferenceSession(\"decoder.onnx\")\n","\n","start = time.time()\n","out_encoder, = sess_encoder.run([\"out\"], {\"x\": x_mel.to('cpu').numpy()})\n","\n","# initialize the tokens\n","tokens = list(tokenizer.sot_sequence_including_notimestamps)\n","\n","next_token = tokenizer.sot\n","while x_tokens.shape[1] <= max_tokens and next_token != tokenizer.eot:\n","    out_decoder, = sess_decoder.run(\n","        [\"out\"],\n","        {\n","            \"tokens\": np.asarray([tokens], dtype=\"int64\"),\n","            \"audio\": out_encoder,\n","        },\n","    )\n","    next_token = out_decoder[0, -1].argmax()\n","    tokens.append(next_token)\n","\n","print(\"took\", time.time() - start, \"seconds\")\n","print(tokenizer.decode(x_tokens[0]))\n","\n","#: PyTorch with kv-caching\n","start = time.time()\n","whisper.decode(\n","    model,\n","    x_mel,\n","    options=whisper.DecodingOptions(\n","        fp16=False,\n","        without_timestamps=True,\n","        suppress_blank=False,\n","        suppress_tokens=[],\n","        language='english'\n","    ),\n",")\n","print(time.time() - start)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/WongKinYiu/yolov7/blob/main/tools/YOLOv7onnx.ipynb","timestamp":1715164177833}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
